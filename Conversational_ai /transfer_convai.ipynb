{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer_convai.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushil79g/60daysUdacity/blob/master/Conversational_ai%20/transfer_convai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqjVKk5DNZ3B",
        "colab_type": "text"
      },
      "source": [
        "### This notebook is optionally accelerated with a GPU runtime.\n",
        "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "# BERT\n",
        "\n",
        "*Author: HuggingFace Team*\n",
        "\n",
        "**Bidirectional Encoder Representations from Transformers.**\n",
        "\n",
        "_ | _\n",
        "- | -\n",
        "![alt](https://pytorch.org/assets/images/bert1.png) | ![alt](https://pytorch.org/assets/images/bert2.png)\n",
        "\n",
        "\n",
        "### Model Description\n",
        "\n",
        "BERT was released together with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin et al. The model is based on the Transformer architecture introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani et al and has led to significant improvements on a wide range of downstream tasks.\n",
        "\n",
        "Here are 8 models based on BERT with [Google's pre-trained models](https://github.com/google-research/bert) along with the associated Tokenizer.\n",
        "It includes:\n",
        "- `bertTokenizer`: perform end-to-end tokenization, i.e. basic tokenization followed by WordPiece tokenization\n",
        "- `bertModel`: raw BERT Transformer model (fully pre-trained)\n",
        "- `bertForMaskedLM`: BERT Transformer with the pre-trained masked language modeling head on top (fully pre-trained)\n",
        "- `bertForNextSentencePrediction`: BERT Transformer with the pre-trained next sentence prediction classifier on top (fully pre-trained)\n",
        "- `bertForPreTraining`: BERT Transformer with masked language modeling head and next sentence prediction classifier on top (fully pre-trained)\n",
        "- `bertForSequenceClassification`: BERT Transformer with a sequence classification head on top (BERT Transformer is pre-trained, the sequence classification head is only initialized and has to be trained)\n",
        "- `bertForMultipleChoice`: BERT Transformer with a multiple choice head on top (used for task like Swag) (BERT Transformer is pre-trained, the multiple choice classification head is only initialized and has to be trained)\n",
        "- `bertForTokenClassification`: BERT Transformer with a token classification head on top (BERT Transformer is pre-trained, the token classification head is only initialized and has to be trained)\n",
        "- `bertForQuestionAnswering`: BERT Transformer with a token classification head on top (BERT Transformer is pre-trained, the token classification head is only initialized and has to be trained)\n",
        "\n",
        "### Requirements\n",
        "\n",
        "Unlike most other PyTorch Hub models, BERT requires a few additional Python packages to be installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Q9fG3XNZ3E",
        "colab_type": "code",
        "outputId": "84472e59-a162-49d5-cd9b-909c8d27e935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "%%bash\n",
        "pip install tqdm boto3 requests regex"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (1.9.189)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Collecting regex\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3) (1.12.189)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.9.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.6.16)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.189->boto3) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py): started\n",
            "  Building wheel for regex (setup.py): finished with status 'done'\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built regex\n",
            "Installing collected packages: regex\n",
            "Successfully installed regex-2019.6.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaNlvvKwNZ3J",
        "colab_type": "text"
      },
      "source": [
        "### Example\n",
        "\n",
        "Here is an example on how to tokenize the input text with `bertTokenizer`, and then get the hidden states computed by `bertModel` or predict masked tokens using `bertForMaskedLM`. The example also includes snippets showcasing how to use `bertForNextSentencePrediction`, `bertForQuestionAnswering`, `bertForSequenceClassification`, `bertForMultipleChoice`, `bertForTokenClassification`, and `bertForPreTraining`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy6XZ_srNZ3J",
        "colab_type": "code",
        "outputId": "5c41ff08-77ec-415f-bb06-74ed2a8f5ca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "### First, tokenize the input\n",
        "import torch\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertTokenizer', 'bert-base-cased', do_basic_tokenize=False)\n",
        "\n",
        "# Tokenized input\n",
        "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
        "tokenized_text = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/huggingface/pytorch-pretrained-BERT/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
            "100%|██████████| 213450/213450 [00:00<00:00, 2415931.60B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmAb9al5NZ3M",
        "colab_type": "code",
        "outputId": "405e5f95-36ee-4a85-8b4a-9fe40693591c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "### Get the hidden states computed by `bertModel`\n",
        "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertModel', 'bert-base-cased')\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n",
            "100%|██████████| 313/313 [00:00<00:00, 117017.31B/s]\n",
            "100%|██████████| 435779157/435779157 [00:12<00:00, 35696160.30B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLg8gPu5NZ3P",
        "colab_type": "code",
        "outputId": "9c33d945-76da-4c12-9139-d8af324ff4ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "### Predict masked tokens using `bertForMaskedLM`\n",
        "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "masked_index = 8\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "maskedLM_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertForMaskedLM', 'bert-base-cased')\n",
        "maskedLM_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = maskedLM_model(tokens_tensor, segments_tensors)\n",
        "\n",
        "# Get the predicted token\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "assert predicted_token == 'Jim'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2b81ead8cd02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get the predicted token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpredicted_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mpredicted_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mpredicted_token\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Jim'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNaaWYIZNZ3S",
        "colab_type": "code",
        "outputId": "76e8dc56-cf74-49fe-c479-9c13fd28a85a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Classify next sentence using ``bertForNextSentencePrediction``\n",
        "# Going back to our initial input\n",
        "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
        "tokenized_text = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "nextSent_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertForNextSentencePrediction', 'bert-base-cased')\n",
        "nextSent_model.eval()\n",
        "\n",
        "# Predict the next sentence classification logits\n",
        "with torch.no_grad():\n",
        "    next_sent_classif_logits = nextSent_model(tokens_tensor, segments_tensors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LfqR9hdNZ3W",
        "colab_type": "code",
        "outputId": "25c60a9c-1a4e-4305-e260-59ad0cc863a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Classify next sentence using ``bertForNextSentencePrediction``\n",
        "nextSent_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertForNextSentencePrediction', 'bert-base-cased')\n",
        "nextSent_model.eval()\n",
        "\n",
        "# Predict the next sentence classification logits\n",
        "with torch.no_grad():\n",
        "    next_sent_classif_logits = nextSent_model(tokens_tensor, segments_tensors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIrkWVkwNZ3Y",
        "colab_type": "code",
        "outputId": "85607a38-c421-44f6-c146-ff3173ffd9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Question answering using `bertForQuestionAnswering`\n",
        "questionAnswering_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertForQuestionAnswering', 'bert-base-cased')\n",
        "questionAnswering_model.eval()\n",
        "\n",
        "# Predict the start and end positions logits\n",
        "with torch.no_grad():\n",
        "    start_logits, end_logits = questionAnswering_model(tokens_tensor, segments_tensors)\n",
        "\n",
        "# Or get the total loss which is the sum of the CrossEntropy loss for the start and end token positions (set model to train mode before if used for training)\n",
        "start_positions, end_positions = torch.tensor([12]), torch.tensor([14])\n",
        "multiple_choice_loss = questionAnswering_model(tokens_tensor, segments_tensors, start_positions=start_positions, end_positions=end_positions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auKcdXUUNZ3b",
        "colab_type": "code",
        "outputId": "928e941e-d478-4268-fc19-9fb2acd5b5bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Classify sequence using `bertForSequenceClassification`\n",
        "# Load bertForSequenceClassification\n",
        "seqClassification_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertForSequenceClassification', 'bert-base-cased', num_labels=2)\n",
        "seqClassification_model.eval()\n",
        "\n",
        "# Predict the sequence classification logits\n",
        "with torch.no_grad():\n",
        "    seq_classif_logits = seqClassification_model(tokens_tensor, segments_tensors)\n",
        "\n",
        "# Or get the sequence classification loss (set model to train mode before if used for training)\n",
        "labels = torch.tensor([1])\n",
        "seq_classif_loss = seqClassification_model(tokens_tensor, segments_tensors, labels=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4MG5DF_NZ3e",
        "colab_type": "code",
        "outputId": "1c93e862-060b-4764-b573-c7033ea99151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Sequence tagging using `bertForTokenClassification`\n",
        "tokClassification_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertForTokenClassification', 'bert-base-cased', num_labels=2)\n",
        "tokClassification_model.eval()\n",
        "# Predict the token classification logits\n",
        "with torch.no_grad():\n",
        "    classif_logits = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "# Or get the token classification loss (set model to train mode before if used for training)\n",
        "labels = torch.tensor([[0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0]])\n",
        "classif_loss = tokClassification_model(tokens_tensor, segments_tensors, labels=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7WI_NLqNZ3h",
        "colab_type": "code",
        "outputId": "1d55b3c4-a6e9-4dab-b56f-942b3ac89e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "### Select answer among multiple choice using `bertForMultipleChoice`\n",
        "multiplChoice_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertForMultipleChoice', 'bert-base-cased', num_choices=2)\n",
        "multiplChoice_model.eval()\n",
        "\n",
        "tokens_tensor = torch.tensor([[indexed_tokens, indexed_tokens]])\n",
        "segments_tensors = torch.tensor([[segments_ids, segments_ids]])\n",
        "\n",
        "# Predict the multiple choice logits\n",
        "with torch.no_grad():\n",
        "    multiple_choice_logits = multiplChoice_model(tokens_tensor, segments_tensors)\n",
        "\n",
        "# Or get the multiple choice loss (set model to train mode before if used for training)\n",
        "labels = torch.tensor([1])\n",
        "multiple_choice_loss = multiplChoice_model(tokens_tensor, segments_tensors, labels=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ef5591e6c8cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmultiplChoice_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huggingface/pytorch-pretrained-BERT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bertForMultipleChoice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_choices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmultiplChoice_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexed_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexed_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msegments_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegments_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(github, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master/hubconfs/bert_hubconf.py\u001b[0m in \u001b[0;36mbertForMultipleChoice\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mmultiple_choice_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set model.train() before if training this loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMultipleChoice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master/pytorch_transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_choices'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6gv9iUdNZ3m",
        "colab_type": "code",
        "outputId": "e279cd58-26ec-426b-8822-3bc6f611ef5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Fine-tune BERT using `bertForPreTraining`\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "forPretraining_model = torch.hub.load('huggingface/pytorch-pretrained-BERT', 'bertForPreTraining', 'bert-base-cased')\n",
        "masked_lm_logits_scores, seq_relationship_logits = forPretraining_model(tokens_tensor, segments_tensors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-pretrained-BERT_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1pe_KaQNZ3q",
        "colab_type": "text"
      },
      "source": [
        "### Resources\n",
        "\n",
        " - Paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
        " - Initial repository (with detailed examples and documentation): [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADzQ02WBPIfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert import OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXOUUnQ-PUst",
        "colab_type": "code",
        "outputId": "d657565d-6dfb-4a22-a660-7aeda637074b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# !pip install pytorch_pretrained_bert\n",
        "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 478750579/478750579 [00:09<00:00, 52993865.18B/s]\n",
            "100%|██████████| 273/273 [00:00<00:00, 47528.02B/s]\n",
            "100%|██████████| 815973/815973 [00:00<00:00, 3869230.47B/s]\n",
            "100%|██████████| 458495/458495 [00:00<00:00, 3420982.02B/s]\n",
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpXXmYRyPXRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N71RwpU1Qn-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.set_special_tokens(SPECIAL_TOKENS)\n",
        "model.set_num_special_tokens(len(SPECIAL_TOKENS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxwVv9cOQx2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import chain\n",
        "persona = [[\"i\",\"like\",\"playing\",\"football\",\".\"],\n",
        "          [\"i\",\"am\",\"from\",\"NYC\",\".\"]]\n",
        "history = [[\"hello\",\"how\",\"are\",\"you\",\"?\"],\n",
        "          [\"i\",\"am\",\"fine\",\"thanks\",\".\"]]\n",
        "reply = [\"great\",\"to\",\"hear\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S3U5i2RRQzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bos, eos, speaker1, speaker2 = \"<bos>\",\"<eos>\",\"<speaker1>\",\"<speaker2>\"\n",
        "def build_inputs(persona, history, reply):\n",
        "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
        "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-1) %2 else speaker1] +s for i,s in enumerate(sequence[1:])]\n",
        "    words = list(chain(*sequence))\n",
        "    segments = [speaker2 if i % 2 else speaker1\n",
        "                for i,s in enumerate(sequence) for _ in s]\n",
        "    position = list(range(len(words)))\n",
        "    return words, segments, position, sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blzgj5S4Sa2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words, segments, position, sequence = build_inputs(persona, history, reply)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUP4ty17SmSO",
        "colab_type": "code",
        "outputId": "f19be303-9b46-4b0e-fbb4-73ddc31f14c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.', '<speaker2>', 'hello', 'how', 'are', 'you', '?', '<speaker2>', 'i', 'am', 'fine', 'thanks', '.', '<speaker2>', 'great', 'to', 'hear', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTdSvv4WTtvq",
        "colab_type": "code",
        "outputId": "cbc279b5-e038-421e-e511-d5eaaf0cd540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(segments)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdDIjs0wTtyn",
        "colab_type": "code",
        "outputId": "244e585b-ceee-441a-ce84-b374bf4977e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(position)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwYGb1mTT1VY",
        "colab_type": "code",
        "outputId": "86be7b9b-f144-4015-fb5e-da6a9983a4e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(sequence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['<bos>', 'i', 'like', 'playing', 'football', '.', 'i', 'am', 'from', 'NYC', '.'], ['<speaker2>', 'hello', 'how', 'are', 'you', '?'], ['<speaker2>', 'i', 'am', 'fine', 'thanks', '.'], ['<speaker2>', 'great', 'to', 'hear', '<eos>']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bgLwcrrSo_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = tokenizer.convert_tokens_to_ids(words)\n",
        "segments = tokenizer.convert_tokens_to_ids(segments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5qpfCzUSzbq",
        "colab_type": "code",
        "outputId": "8541130a-6c11-4ef9-dc80-449c9b219e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[40478, 11, 14594, 0, 0, 1, 11, 1574, 0, 0, 1, 40481, 0, 1991, 2183, 7159, 19, 40481, 11, 1574, 0, 12389, 1, 40481, 5201, 571, 863, 40479]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMkAIindTmFv",
        "colab_type": "code",
        "outputId": "af760ffb-478f-4046-8562-8697a9f92600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(segments)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[40480, 40480, 40480, 40480, 40480, 40480, 40480, 40480, 40480, 40480, 40480, 40481, 40481, 40481, 40481, 40481, 40481, 40480, 40480, 40480, 40480, 40480, 40480, 40481, 40481, 40481, 40481, 40481]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6leWXzSZRBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "# Let's add a distractor to our previously defined persona, history and reply\n",
        "distractor = [\"sorry\", \"to\", \"hear\", \"that\"]\n",
        "\n",
        "# Build & tokenize inputs ending with our distractor like we did with the gold reply\n",
        "words_distractor, segments_distractor, _, _ = build_inputs(persona, history, distractor)\n",
        "words_distractor = tokenizer.convert_tokens_to_ids(words_distractor)\n",
        "segments_distractor = tokenizer.convert_tokens_to_ids(segments_distractor)\n",
        "\n",
        "# Prepare our language modeling targets: keep only the reply segment, -1 on the rest\n",
        "lm_targets = ([-1] * sum(len(s) for s in sequence[:-1])) \\\n",
        "             + [-1] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
        "lm_distractor = [-1] * len(words_distractor)\n",
        "\n",
        "# Store the position of the last tokens for the next-sentence prediction loss\n",
        "last_token = len(words) - 1\n",
        "last_token_distractor = len(words_distractor) - 1\n",
        "\n",
        "# Now we can pad reply and distractor inputs and targets to the same length\n",
        "padding_length = max(len(words), len(words_distractor))\n",
        "def pad(x, padding):\n",
        "    return x + [padding] * (padding_length - len(x))\n",
        "\n",
        "(words, words_distractor,\n",
        " segments, segments_distractor) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
        "                                   for x in (words, words_distractor,\n",
        "                                             segments, segments_distractor)]\n",
        "\n",
        "(lm_targets, lm_distractor) = [pad(x, -1) for x in (lm_targets, lm_distractor)]\n",
        " \n",
        "# And gather reply and distractor inputs to build the input tensors:\n",
        "# words tokens\n",
        "input_ids = torch.tensor([[words, words_distractor]], dtype=torch.long)\n",
        "# segment tokens\n",
        "token_type_ids = torch.tensor([[segments, segments_distractor]], dtype=torch.long)\n",
        "# Positions tokens can be automatically created by the model as (0, 1, ..., N)\n",
        "# Last tokens location\n",
        "mc_token_ids = torch.tensor([[last_token, last_token_distractor]], dtype=torch.long)\n",
        "# Language modeling labels\n",
        "lm_labels = torch.tensor([[lm_targets, lm_distractor]], dtype=torch.long)\n",
        "# Next-sentence prediction labels\n",
        "mc_labels = torch.tensor([0], dtype=torch.long)  # Gold reply is 1st (index 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i80LF0N1ZZoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm_loss, mc_loss = model(input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids)\n",
        "\n",
        "# Total loss as a weighted sum\n",
        "lm_coef = 2.0\n",
        "mc_coef = 1.0\n",
        "total_loss = lm_loss * lm_coef + mc_loss * mc_coef"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW2uGpkia4KM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import json\n",
        "# import logging\n",
        "# import os\n",
        "# import tarfile\n",
        "# import tempfile\n",
        "\n",
        "# import torch\n",
        "\n",
        "# from pytorch_pretrained_bert import cached_path\n",
        "\n",
        "# PERSONACHAT_URL = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "# HF_FINETUNED_MODEL = \"https://s3.amazonaws.com/models.huggingface.co/transfer-learning-chatbot/finetuned_chatbot_gpt.tar.gz\"\n",
        "\n",
        "# # logger = logging.getLogger(__file__)\n",
        "\n",
        "# def download_pretrained_model():\n",
        "#     \"\"\" Download and extract finetuned model from S3 \"\"\"\n",
        "#     resolved_archive_file = cached_path(HF_FINETUNED_MODEL)\n",
        "#     tempdir = tempfile.mkdtemp()\n",
        "\n",
        "#     logger.info(\"extracting archive file {} to temp dir {}\".format(resolved_archive_file, tempdir))\n",
        "#     with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
        "#         archive.extractall(tempdir)\n",
        "#     return tempdir\n",
        "\n",
        "\n",
        "# def get_dataset(tokenizer, dataset_path, dataset_cache=None):\n",
        "#     \"\"\" Get PERSONACHAT from S3 \"\"\"\n",
        "#     dataset_path = dataset_path or PERSONACHAT_URL\n",
        "#     dataset_cache = dataset_cache + '_' + type(tokenizer).__name__  # Do avoid using GPT cache for GPT-2 and vice-versa\n",
        "#     if dataset_cache and os.path.isfile(dataset_cache):\n",
        "#         logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
        "#         dataset = torch.load(dataset_cache)\n",
        "#     else:\n",
        "#         logger.info(\"Download dataset from %s\", dataset_path)\n",
        "#         personachat_file = cached_path(dataset_path)\n",
        "#         with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
        "#             dataset = json.loads(f.read())\n",
        "\n",
        "#         logger.info(\"Tokenize and encode the dataset\")\n",
        "#         def tokenize(obj):\n",
        "#             if isinstance(obj, str):\n",
        "#                 return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "#             if isinstance(obj, dict):\n",
        "#                 return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "#             return list(tokenize(o) for o in obj)\n",
        "#         dataset = tokenize(dataset)\n",
        "#         if dataset_cache:\n",
        "#             torch.save(dataset, dataset_cache)\n",
        "#     return dataset\n",
        "\n",
        "# def get_dataset_personalities(tokenizer, dataset_path, dataset_cache=None):\n",
        "#     \"\"\" Get personalities from PERSONACHAT \"\"\"\n",
        "#     dataset_path = dataset_path or PERSONACHAT_URL\n",
        "#     dataset_cache = dataset_cache + '_' + type(tokenizer).__name__  # Do avoid using GPT cache for GPT-2 and vice-versa\n",
        "#     if os.path.isfile(dataset_cache):\n",
        "#         logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
        "#         personachat = torch.load(dataset_cache)\n",
        "#     else:\n",
        "#         logger.info(\"Download PERSONACHAT dataset from %s\", dataset_path)\n",
        "#         personachat_file = cached_path(dataset_path)\n",
        "#         with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
        "#             personachat = json.loads(f.read())\n",
        "\n",
        "#         logger.info(\"Tokenize and encode the dataset\")\n",
        "#         def tokenize(obj):\n",
        "#             if isinstance(obj, str):\n",
        "#                 return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "#             if isinstance(obj, dict):\n",
        "#                 return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "#             return list(tokenize(o) for o in obj)\n",
        "#         personachat = tokenize(personachat)\n",
        "#         torch.save(personachat, dataset_cache)\n",
        "\n",
        "#     logger.info(\"Filter personalities\")\n",
        "#     personalities = []\n",
        "#     for dataset in personachat.values():\n",
        "#         for dialog in dataset:\n",
        "#             personalities.append(dialog[\"personality\"])\n",
        "\n",
        "#     logger.info(\"Gathered {} personalities\".format(len(personalities)))\n",
        "#     return personalities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6lJj1ZdZcAh",
        "colab_type": "code",
        "outputId": "3b7f9d0e-e240-47c5-854d-23124b610d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import json\n",
        "from pytorch_pretrained_bert import cached_path\n",
        "\n",
        "url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "# Download and load JSON dataset\n",
        "personachat_file = cached_path(url)\n",
        "with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "# Tokenize and encode the dataset using our loaded GPT tokenizer\n",
        "def tokenize(obj):\n",
        "    if isinstance(obj, str):\n",
        "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
        "    if isinstance(obj, dict):\n",
        "        return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "    return list(tokenize(o) for o in obj)\n",
        " \n",
        "dataset = tokenize(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 209850483/209850483 [00:03<00:00, 56462596.02B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKBidULAZfDG",
        "colab_type": "code",
        "outputId": "93263b9b-5ae3-4d7e-a7bd-d7cfd359fa20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "# Here is how to use this function for top-p sampling\n",
        "temperature = 1.0\n",
        "top_k = 0\n",
        "top_p = 0.9\n",
        "\n",
        "# Get logits with a forward pass in our model (input is pre-defined)\n",
        "logits = model(input)\n",
        "\n",
        "# Keep only the last token predictions of the first batch item (batch size 1), apply a temperature coefficient and filter\n",
        "logits = logits[0, -1, :] / temperature\n",
        "filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "\n",
        "# Sample from the filtered distribution\n",
        "probabilities = F.softmax(filtered_logits, dim=-1)\n",
        "next_token = torch.multinomial(probabilities, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-180b95e499e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Get logits with a forward pass in our model (input is pre-defined)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Keep only the last token predictions of the first batch item (batch size 1), apply a temperature coefficient and filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'mc_token_ids'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etHs2bmycZiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}