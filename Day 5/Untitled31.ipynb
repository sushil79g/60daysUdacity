{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "noQ-yHsJoEsw",
        "colab_type": "code",
        "outputId": "40bda9d6-45c9-42df-b278-9bc8389c337f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 45.0MB/s \n",
            "\u001b[?25hCollecting regex (from pytorch-pretrained-bert)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.175)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.175 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.175)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.175->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.175->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.175->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built regex\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.6.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdT433Pqn5E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert import OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rc_1uJGoXzf",
        "colab_type": "code",
        "outputId": "3a762c2e-928c-4449-d808-dd7c94fa9582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 478750579/478750579 [00:30<00:00, 15832010.59B/s]\n",
            "100%|██████████| 273/273 [00:00<00:00, 74581.19B/s]\n",
            "100%|██████████| 815973/815973 [00:00<00:00, 1021738.82B/s]\n",
            "100%|██████████| 458495/458495 [00:00<00:00, 750677.43B/s]\n",
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Jw0ZK2oZ28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
        "tokenizer.set_special_tokens(SPECIAL_TOKENS)\n",
        "model.set_num_special_tokens(len(SPECIAL_TOKENS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICwBsNpHo3-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import chain\n",
        "# Let's define our contexts and special tokens\n",
        "persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
        "           [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
        "history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
        "           [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
        "reply = [\"great\", \"to\", \"hear\"]\n",
        "bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTK0e45Jo_P7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_inputs(persona, history, reply):\n",
        "    # Build our sequence by adding delimiters and concatenating\n",
        "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
        "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
        "                                for i, s in enumerate(sequence[1:])]\n",
        "    # Build our word, segments and position inputs from the sequence\n",
        "    words = list(chain(*sequence))                          # word tokens\n",
        "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
        "                for i, s in enumerate(sequence) for _ in s]\n",
        "    position = list(range(len(words)))                      # position tokens\n",
        "    return words, segments, position, sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01G9vDaPpBd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words, segments, position, sequence = build_inputs(persona, history, reply)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBRTea4XpD9M",
        "colab_type": "code",
        "outputId": "8b3234c1-7312-460e-98d3-91a4e2d6c831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from pprint import pprint\n",
        "pprint(sequence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['<bos>',\n",
            "  'i',\n",
            "  'like',\n",
            "  'playing',\n",
            "  'football',\n",
            "  '.',\n",
            "  'i',\n",
            "  'am',\n",
            "  'from',\n",
            "  'NYC',\n",
            "  '.'],\n",
            " ['<speaker1>', 'hello', 'how', 'are', 'you', '?'],\n",
            " ['<speaker2>', 'i', 'am', 'fine', 'thanks', '.'],\n",
            " ['<speaker1>', 'great', 'to', 'hear', '<eos>']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy06PIIFpGwn",
        "colab_type": "code",
        "outputId": "443f59e4-aea6-4b6f-ce7e-38f7a441e360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# Let's add a distractor to our previously defined persona, history and reply\n",
        "distractor = [\"sorry\", \"to\", \"hear\", \"that\"]\n",
        "\n",
        "# Build & tokenize inputs ending with our distractor like we did with the gold reply\n",
        "words_distractor, segments_distractor, _, _ = build_inputs(persona, history, distractor)\n",
        "words_distractor = tokenizer.convert_tokens_to_ids(words_distractor)\n",
        "segments_distractor = tokenizer.convert_tokens_to_ids(segments_distractor)\n",
        "\n",
        "# Prepare our language modeling targets: keep only the reply segment, -1 on the rest\n",
        "lm_targets = ([-1] * sum(len(s) for s in sequence[:-1])) \\\n",
        "             + [-1] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
        "lm_distractor = [-1] * len(words_distractor)\n",
        "\n",
        "# Store the position of the last tokens for the next-sentence prediction loss\n",
        "last_token = len(words) - 1\n",
        "last_token_distractor = len(words_distractor) - 1\n",
        "\n",
        "# Now we can pad reply and distractor inputs and targets to the same length\n",
        "padding_length = max(len(words), len(words_distractor))\n",
        "def pad(x, padding):\n",
        "    return x + [padding] * (padding_length - len(x))\n",
        "\n",
        "(words, words_distractor,\n",
        " segments, segments_distractor) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'))\n",
        "                                   for x in (words, words_distractor,\n",
        "                                             segments, segments_distractor)]\n",
        "\n",
        "(lm_targets, lm_distractor) = [pad(x, -1) for x in (lm_targets, lm_distractor)]\n",
        " \n",
        "# And gather reply and distractor inputs to build the input tensors:\n",
        "# words tokens\n",
        "input_ids = torch.tensor([[words, words_distractor]], dtype=torch.long)\n",
        "# segment tokens\n",
        "token_type_ids = torch.tensor([[segments, segments_distractor]], dtype=torch.long)\n",
        "# Positions tokens can be automatically created by the model as (0, 1, ..., N)\n",
        "# Last tokens location\n",
        "mc_token_ids = torch.tensor([[last_token, last_token_distractor]], dtype=torch.long)\n",
        "# Language modeling labels\n",
        "lm_labels = torch.tensor([[lm_targets, lm_distractor]], dtype=torch.long)\n",
        "# Next-sentence prediction labels\n",
        "mc_labels = torch.tensor([0], dtype=torch.long)  # Gold reply is 1st (index 0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a4187fe0f91d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# And gather reply and distractor inputs to build the input tensors:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# words tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_distractor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m# segment tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_distractor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8N82NLIpeHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}